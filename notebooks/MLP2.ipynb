{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e5e0b38",
   "metadata": {},
   "source": [
    "### importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f623f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52de9381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6122b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324353a8",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79e31322",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reload raw data cleanly without scale and drop\n",
    "\n",
    "df = pd.read_csv(\"../data/raw/creditcard.csv\")\n",
    "df = df.sort_values(\"Time\").reset_index(drop=True)\n",
    "\n",
    "features = [c for c in df.columns if c != \"Class\"]\n",
    "X = df[features]\n",
    "y = df[\"Class\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a8ee57",
   "metadata": {},
   "source": [
    "### Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66ad48fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing values: 0\n",
      "Duplicated rows: 1081\n",
      "Dropped duplicates. New shape: (283726, 31)\n"
     ]
    }
   ],
   "source": [
    "missing_total = df.isnull().sum().sum()\n",
    "dup_count = df.duplicated().sum()\n",
    "\n",
    "print(\"Total missing values:\", missing_total)\n",
    "print(\"Duplicated rows:\", dup_count)\n",
    "\n",
    "if dup_count > 0:\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    print(\"Dropped duplicates. New shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a4bfef",
   "metadata": {},
   "source": [
    "### checking for class imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bb7dcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts:\n",
      " Class\n",
      "0    283253\n",
      "1       473\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class %:\n",
      " Class\n",
      "0    99.8333\n",
      "1     0.1667\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "counts = df[\"Class\"].value_counts()\n",
    "pct = df[\"Class\"].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Class counts:\\n\", counts)\n",
    "print(\"\\nClass %:\\n\", pct.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d6362e",
   "metadata": {},
   "source": [
    "### Time-based split (train/val/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d697dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "  Train       X: (198608, 30)  y: (198608,)\n",
      "  Validation  X: (42559, 30)  y: (42559,)\n",
      "  Test        X: (42559, 30)  y: (42559,)\n",
      "\n",
      "Fraud rate:\n",
      "  Train:      0.001843\n",
      "  Validation: 0.001292\n",
      "  Test:       0.001222\n",
      "\n",
      "Fraud counts:\n",
      "  Train:      366 / 198608\n",
      "  Validation: 55 / 42559\n",
      "  Test:       52 / 42559\n"
     ]
    }
   ],
   "source": [
    "n = len(df)\n",
    "train_end = int(n * 0.70)\n",
    "val_end = int(n * 0.85)\n",
    "\n",
    "train_df = df.iloc[:train_end]\n",
    "val_df = df.iloc[train_end:val_end]\n",
    "test_df = df.iloc[val_end:]\n",
    "\n",
    "X_train = train_df.drop(columns=[\"Class\"])\n",
    "y_train = train_df[\"Class\"].astype(int)\n",
    "\n",
    "X_val = val_df.drop(columns=[\"Class\"])\n",
    "y_val = val_df[\"Class\"].astype(int)\n",
    "\n",
    "X_test = test_df.drop(columns=[\"Class\"])\n",
    "y_test = test_df[\"Class\"].astype(int)\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"  Train       X:\", X_train.shape, \" y:\", y_train.shape)\n",
    "print(\"  Validation  X:\", X_val.shape,   \" y:\", y_val.shape)\n",
    "print(\"  Test        X:\", X_test.shape,  \" y:\", y_test.shape)\n",
    "\n",
    "print(\"\\nFraud rate:\")\n",
    "print(f\"  Train:      {y_train.mean():.6f}\")\n",
    "print(f\"  Validation: {y_val.mean():.6f}\")\n",
    "print(f\"  Test:       {y_test.mean():.6f}\")\n",
    "\n",
    "print(\"\\nFraud counts:\")\n",
    "print(f\"  Train:      {int(y_train.sum())} / {len(y_train)}\")\n",
    "print(f\"  Validation: {int(y_val.sum())} / {len(y_val)}\")\n",
    "print(f\"  Test:       {int(y_test.sum())} / {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724b7d35",
   "metadata": {},
   "source": [
    "### Undersampling (train only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db688aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train class counts BEFORE undersampling:\n",
      "Class\n",
      "0    198242\n",
      "1       366\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train class counts AFTER undersampling:\n",
      "Class\n",
      "0    366\n",
      "1    366\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.concat([X_train, y_train.rename(\"Class\")], axis=1)\n",
    "\n",
    "fraud_df = train_data[train_data[\"Class\"] == 1]\n",
    "non_fraud_df = train_data[train_data[\"Class\"] == 0]\n",
    "\n",
    "# Safety check\n",
    "print(\"\\nTrain class counts BEFORE undersampling:\")\n",
    "print(train_data[\"Class\"].value_counts())\n",
    "\n",
    "non_fraud_sampled = non_fraud_df.sample(n=len(fraud_df), random_state=42)\n",
    "\n",
    "balanced_train = (\n",
    "    pd.concat([fraud_df, non_fraud_sampled])\n",
    "      .sample(frac=1, random_state=42)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "X_train_under = balanced_train.drop(columns=[\"Class\"])\n",
    "y_train_under = balanced_train[\"Class\"].astype(int)\n",
    "\n",
    "print(\"\\nTrain class counts AFTER undersampling:\")\n",
    "print(y_train_under.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0904bf",
   "metadata": {},
   "source": [
    "### Common evaluation helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8155fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, X, y, name=\"Model\"):\n",
    "    probs = model.predict_proba(X)[:, 1]\n",
    "    ap = average_precision_score(y, probs)\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(f\"PR-AUC (Average Precision): {ap:.4f}\")\n",
    "    return ap, probs\n",
    "\n",
    "def evaluate_at_threshold(model, X, y, name=\"Model\", threshold=0.5):\n",
    "    probs = model.predict_proba(X)[:, 1]\n",
    "    ap = average_precision_score(y, probs)\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "\n",
    "    prec = precision_score(y, preds, zero_division=0)\n",
    "    rec = recall_score(y, preds, zero_division=0)\n",
    "    f1 = f1_score(y, preds, zero_division=0)\n",
    "\n",
    "    print(f\"\\n--- {name} (threshold={threshold}) ---\")\n",
    "    print(f\"PR-AUC: {ap:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y, preds))\n",
    "    return ap, prec, rec, f1\n",
    "\n",
    "def evaluate_at_top_n(model, X, y, name=\"Model\", top_n=200):\n",
    "    probs = model.predict_proba(X)[:, 1]\n",
    "    ap = average_precision_score(y, probs)\n",
    "\n",
    "    idx = np.argsort(probs)[::-1]\n",
    "    preds = np.zeros(len(y), dtype=int)\n",
    "    preds[idx[:top_n]] = 1\n",
    "\n",
    "    y_true = np.array(y)\n",
    "    tp = ((preds == 1) & (y_true == 1)).sum()\n",
    "    fp = ((preds == 1) & (y_true == 0)).sum()\n",
    "    fn = ((preds == 0) & (y_true == 1)).sum()\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    print(f\"\\n--- {name} (Top-N={top_n}) ---\")\n",
    "    print(f\"PR-AUC: {ap:.4f}\")\n",
    "    print(f\"Precision@Top{top_n}: {precision:.4f} | Recall@Top{top_n}: {recall:.4f} | F1@Top{top_n}: {f1:.4f}\")\n",
    "    return ap, precision, recall, f1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f110763",
   "metadata": {},
   "source": [
    "### Preprocessing: scale only Amount \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "253b3a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_cols = [\"Amount\"]  # optionally add \"Time\"\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[(\"scale\", RobustScaler(), scale_cols)],\n",
    "    remainder=\"passthrough\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253da27d",
   "metadata": {},
   "source": [
    "### fraud detection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf837f",
   "metadata": {},
   "source": [
    "#### logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ae4170f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logistic Regression (Validation) ---\n",
      "PR-AUC (Average Precision): 0.4039\n",
      "\n",
      "--- LogReg (Under+ScaleAmount) (threshold=0.05) ---\n",
      "PR-AUC: 0.4039\n",
      "Precision: 0.0038 | Recall: 0.9818 | F1: 0.0076\n",
      "Confusion Matrix:\n",
      " [[28427 14077]\n",
      " [    1    54]]\n",
      "\n",
      "--- LogReg (Under+ScaleAmount) (Top-N=200) ---\n",
      "PR-AUC: 0.4039\n",
      "Precision@Top200: 0.2400 | Recall@Top200: 0.8727 | F1@Top200: 0.3765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4039047652584162,\n",
       " np.float64(0.24),\n",
       " np.float64(0.8727272727272727),\n",
       " np.float64(0.3764705882352941))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# Train LR on undersampled training set, with scaling inside pipeline\n",
    "lr_model = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"lr\", LogisticRegression(max_iter=5000, random_state=42))\n",
    "])\n",
    "\n",
    "lr_model.fit(X_train_under, y_train_under)\n",
    "\n",
    "val_probs = lr_model.predict_proba(X_val)[:, 1]\n",
    "val_ap = average_precision_score(y_val, val_probs)\n",
    "\n",
    "print(\"--- Logistic Regression (Validation) ---\")\n",
    "print(f\"PR-AUC (Average Precision): {val_ap:.4f}\")\n",
    "\n",
    "evaluate_at_threshold(lr_model, X_val, y_val, name=\"LogReg (Under+ScaleAmount)\", threshold=0.05)\n",
    "evaluate_at_top_n(lr_model, X_val, y_val, name=\"LogReg (Under+ScaleAmount)\", top_n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4921d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Features by Absolute Coefficient:\n",
      "   Feature  Coefficient  Abs_Coef\n",
      "15     V15    -0.926582  0.926582\n",
      "5       V5     0.862760  0.862760\n",
      "9       V9    -0.840034  0.840034\n",
      "13     V13    -0.815987  0.815987\n",
      "23     V23     0.748007  0.748007\n"
     ]
    }
   ],
   "source": [
    "## Feature analysis (coefficients) with pipeline\n",
    "feature_names = X_train_under.columns\n",
    "coefs = lr_model.named_steps[\"lr\"].coef_[0]\n",
    "\n",
    "lr_coefs = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Coefficient\": coefs\n",
    "})\n",
    "lr_coefs[\"Abs_Coef\"] = lr_coefs[\"Coefficient\"].abs()\n",
    "lr_coefs = lr_coefs.sort_values(\"Abs_Coef\", ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 Features by Absolute Coefficient:\")\n",
    "print(lr_coefs.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53315a5",
   "metadata": {},
   "source": [
    "### Randm Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "951a9872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RF (UnderPipe) (threshold=0.05) ---\n",
      "PR-AUC: 0.8413\n",
      "Precision: 0.0019 | Recall: 1.0000 | F1: 0.0037\n",
      "Confusion Matrix:\n",
      " [[13242 29262]\n",
      " [    0    55]]\n",
      "\n",
      "--- RF (UnderPipe) (Top-N=200) ---\n",
      "PR-AUC: 0.8413\n",
      "Precision@Top200: 0.2400 | Recall@Top200: 0.8727 | F1@Top200: 0.3765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8413424738193728,\n",
       " np.float64(0.24),\n",
       " np.float64(0.8727272727272727),\n",
       " np.float64(0.3764705882352941))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rf_under_pipe = ImbPipeline([\n",
    "    (\"under\", RandomUnderSampler(random_state=42)),\n",
    "    (\"rf\", RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "rf_under_pipe.fit(X_train, y_train)\n",
    "\n",
    "evaluate_at_threshold(rf_under_pipe, X_val, y_val, name=\"RF (UnderPipe)\", threshold=0.05)\n",
    "evaluate_at_top_n(rf_under_pipe, X_val, y_val, name=\"RF (UnderPipe)\", top_n=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287efba9",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b65e838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- XGBoost (Weighted) Validation ---\n",
      "PR-AUC (Average Precision): 0.8514\n",
      "\n",
      "--- XGB (Weighted) (Top-N=200) ---\n",
      "PR-AUC: 0.8514\n",
      "Precision@Top200: 0.2400 | Recall@Top200: 0.8727 | F1@Top200: 0.3765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.851364563837334,\n",
       " np.float64(0.24),\n",
       " np.float64(0.8727272727272727),\n",
       " np.float64(0.3764705882352941))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "neg = (y_train == 0).sum()\n",
    "pos = (y_train == 1).sum()\n",
    "scale_pos_weight = neg / pos\n",
    "\n",
    "xgb_weighted = XGBClassifier(\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    n_estimators=800,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "\n",
    "xgb_weighted.fit(X_train, y_train)\n",
    "\n",
    "val_probs = xgb_weighted.predict_proba(X_val)[:, 1]\n",
    "val_ap = average_precision_score(y_val, val_probs)\n",
    "\n",
    "print(\"--- XGBoost (Weighted) Validation ---\")\n",
    "print(f\"PR-AUC (Average Precision): {val_ap:.4f}\")\n",
    "\n",
    "evaluate_at_top_n(xgb_weighted, X_val, y_val, name=\"XGB (Weighted)\", top_n=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "99c6ae24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Feature Importances (XGBoost Weighted):\n",
      "   Feature  Importance\n",
      "14     V14    0.355702\n",
      "10     V10    0.151346\n",
      "12     V12    0.073023\n",
      "4       V4    0.072063\n",
      "8       V8    0.030004\n"
     ]
    }
   ],
   "source": [
    "## feature importance\n",
    "xgb_importances = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns,\n",
    "    \"Importance\": xgb_weighted.feature_importances_\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 Feature Importances (XGBoost Weighted):\")\n",
    "print(xgb_importances.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be76227",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1197ec45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Naive Bayes (Validation) ---\n",
      "Average Precision (PR-AUC): 0.0539\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     42504\n",
      "           1       0.05      0.76      0.09        55\n",
      "\n",
      "    accuracy                           0.98     42559\n",
      "   macro avg       0.52      0.87      0.54     42559\n",
      "weighted avg       1.00      0.98      0.99     42559\n",
      "\n",
      "\n",
      "--- GNB (Under) (threshold=0.05) ---\n",
      "PR-AUC: 0.0539\n",
      "Precision: 0.0433 | Recall: 0.8000 | F1: 0.0822\n",
      "Confusion Matrix:\n",
      " [[41532   972]\n",
      " [   11    44]]\n",
      "\n",
      "--- GNB (Under) (Top-N=200) ---\n",
      "PR-AUC: 0.0539\n",
      "Precision@Top200: 0.0700 | Recall@Top200: 0.2545 | F1@Top200: 0.1098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.05385767697945146,\n",
       " np.float64(0.07),\n",
       " np.float64(0.2545454545454545),\n",
       " np.float64(0.10980392156862745))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, average_precision_score\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_under, y_train_under)\n",
    "\n",
    "y_val_pred_gnb = gnb.predict(X_val)\n",
    "y_val_probs_gnb = gnb.predict_proba(X_val)[:, 1]\n",
    "\n",
    "val_ap_gnb = average_precision_score(y_val, y_val_probs_gnb)\n",
    "\n",
    "print(\"--- Naive Bayes (Validation) ---\")\n",
    "print(f\"Average Precision (PR-AUC): {val_ap_gnb:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_val, y_val_pred_gnb))\n",
    "\n",
    "# Optional: keep consistent with your other models\n",
    "evaluate_at_threshold(gnb, X_val, y_val, name=\"GNB (Under)\", threshold=0.05)\n",
    "evaluate_at_top_n(gnb, X_val, y_val, name=\"GNB (Under)\", top_n=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73477127",
   "metadata": {},
   "source": [
    "### Over Sampling (SMOTE) + Logistic Regression pipeline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b4b5153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logistic Regression (SMOTE Pipeline) Validation ---\n",
      "PR-AUC (Average Precision): 0.8208\n",
      "\n",
      "--- LogReg (SMOTE) (threshold=0.05) ---\n",
      "PR-AUC: 0.8208\n",
      "Precision: 0.0293 | Recall: 0.9091 | F1: 0.0569\n",
      "Confusion Matrix:\n",
      " [[40850  1654]\n",
      " [    5    50]]\n",
      "\n",
      "--- LogReg (SMOTE) (Top-N=200) ---\n",
      "PR-AUC: 0.8208\n",
      "Precision@Top200: 0.2400 | Recall@Top200: 0.8727 | F1@Top200: 0.3765\n",
      "\n",
      "Top 5 Features by Absolute Coefficient (SMOTE LR):\n",
      "   Feature  Coefficient  Abs_Coef\n",
      "15     V15    -0.396654  0.396654\n",
      "4       V4    -0.277498  0.277498\n",
      "13     V13    -0.234228  0.234228\n",
      "5       V5     0.233138  0.233138\n",
      "11     V11    -0.208442  0.208442\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# SMOTE + Scaling + Logistic Regression in one pipeline\n",
    "lr_smote_pipe = ImbPipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"lr\", LogisticRegression(max_iter=5000, solver=\"liblinear\", random_state=42))\n",
    "])\n",
    "\n",
    "# Fit on training data only\n",
    "lr_smote_pipe.fit(X_train, y_train)\n",
    "\n",
    "# PR-AUC on validation\n",
    "val_probs = lr_smote_pipe.predict_proba(X_val)[:, 1]\n",
    "val_ap = average_precision_score(y_val, val_probs)\n",
    "\n",
    "print(\"--- Logistic Regression (SMOTE Pipeline) Validation ---\")\n",
    "print(f\"PR-AUC (Average Precision): {val_ap:.4f}\")\n",
    "\n",
    "# Threshold / Top-N evaluation\n",
    "evaluate_at_threshold(lr_smote_pipe, X_val, y_val, name=\"LogReg (SMOTE)\", threshold=0.05)\n",
    "evaluate_at_top_n(lr_smote_pipe, X_val, y_val, name=\"LogReg (SMOTE)\", top_n=200)\n",
    "\n",
    "# Feature coefficients (from pipeline)\n",
    "coefs = lr_smote_pipe.named_steps[\"lr\"].coef_[0]\n",
    "lr_sm_coefs = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns,\n",
    "    \"Coefficient\": coefs\n",
    "})\n",
    "lr_sm_coefs[\"Abs_Coef\"] = lr_sm_coefs[\"Coefficient\"].abs()\n",
    "lr_sm_coefs = lr_sm_coefs.sort_values(\"Abs_Coef\", ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 Features by Absolute Coefficient (SMOTE LR):\")\n",
    "print(lr_sm_coefs.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e40c0e",
   "metadata": {},
   "source": [
    "### Random Forest (with SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e08a8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Random Forest (SMOTE Pipeline) Validation ---\n",
      "PR-AUC (Average Precision): 0.8506\n",
      "\n",
      "--- RF (SMOTE) (threshold=0.05) ---\n",
      "PR-AUC: 0.8506\n",
      "Precision: 0.0099 | Recall: 0.9273 | F1: 0.0195\n",
      "Confusion Matrix:\n",
      " [[37390  5114]\n",
      " [    4    51]]\n",
      "\n",
      "--- RF (SMOTE) (Top-N=200) ---\n",
      "PR-AUC: 0.8506\n",
      "Precision@Top200: 0.2450 | Recall@Top200: 0.8909 | F1@Top200: 0.3843\n",
      "\n",
      "Top 5 Feature Importances (RF SMOTE):\n",
      "   Feature  Importance\n",
      "14     V14    0.192877\n",
      "10     V10    0.139928\n",
      "12     V12    0.118468\n",
      "4       V4    0.112564\n",
      "17     V17    0.096249\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "rf_smote_pipe = ImbPipeline(steps=[\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"rf\", RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=10,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "rf_smote_pipe.fit(X_train, y_train)\n",
    "\n",
    "val_probs = rf_smote_pipe.predict_proba(X_val)[:, 1]\n",
    "val_ap = average_precision_score(y_val, val_probs)\n",
    "\n",
    "print(\"--- Random Forest (SMOTE Pipeline) Validation ---\")\n",
    "print(f\"PR-AUC (Average Precision): {val_ap:.4f}\")\n",
    "\n",
    "evaluate_at_threshold(rf_smote_pipe, X_val, y_val, name=\"RF (SMOTE)\", threshold=0.05)\n",
    "evaluate_at_top_n(rf_smote_pipe, X_val, y_val, name=\"RF (SMOTE)\", top_n=200)\n",
    "\n",
    "# Feature importance (from RF inside pipeline)\n",
    "rf_step = rf_smote_pipe.named_steps[\"rf\"]\n",
    "rf_sm_importances = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns,\n",
    "    \"Importance\": rf_step.feature_importances_\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 Feature Importances (RF SMOTE):\")\n",
    "print(rf_sm_importances.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f95b2b5",
   "metadata": {},
   "source": [
    "### XGBoost (with SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16554b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- XGBoost (SMOTE Pipeline) Validation ---\n",
      "PR-AUC (Average Precision): 0.8328\n",
      "\n",
      "--- XGB (SMOTE) (threshold=0.05) ---\n",
      "PR-AUC: 0.8328\n",
      "Precision: 0.2260 | Recall: 0.8545 | F1: 0.3574\n",
      "Confusion Matrix:\n",
      " [[42343   161]\n",
      " [    8    47]]\n",
      "\n",
      "--- XGB (SMOTE) (Top-N=200) ---\n",
      "PR-AUC: 0.8328\n",
      "Precision@Top200: 0.2350 | Recall@Top200: 0.8545 | F1@Top200: 0.3686\n",
      "\n",
      "Top 5 Feature Importances (XGB SMOTE):\n",
      "   Feature  Importance\n",
      "14     V14    0.440862\n",
      "10     V10    0.194849\n",
      "12     V12    0.066844\n",
      "17     V17    0.054192\n",
      "4       V4    0.048823\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "xgb_smote_pipe = ImbPipeline(steps=[\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"xgb\", XGBClassifier(\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=42,\n",
    "        n_estimators=800,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "xgb_smote_pipe.fit(X_train, y_train)\n",
    "\n",
    "val_probs = xgb_smote_pipe.predict_proba(X_val)[:, 1]\n",
    "val_ap = average_precision_score(y_val, val_probs)\n",
    "\n",
    "print(\"--- XGBoost (SMOTE Pipeline) Validation ---\")\n",
    "print(f\"PR-AUC (Average Precision): {val_ap:.4f}\")\n",
    "\n",
    "evaluate_at_threshold(xgb_smote_pipe, X_val, y_val, name=\"XGB (SMOTE)\", threshold=0.05)\n",
    "evaluate_at_top_n(xgb_smote_pipe, X_val, y_val, name=\"XGB (SMOTE)\", top_n=200)\n",
    "\n",
    "# Feature importance (from XGB inside pipeline)\n",
    "xgb_step = xgb_smote_pipe.named_steps[\"xgb\"]\n",
    "xgb_sm_importances = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns,\n",
    "    \"Importance\": xgb_step.feature_importances_\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 Feature Importances (XGB SMOTE):\")\n",
    "print(xgb_sm_importances.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d9339c",
   "metadata": {},
   "source": [
    "### Pipeline for XGBOOST + SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14e5689f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FINAL PIPELINE (XGB+SMOTE): VALIDATION ---\n",
      "PR-AUC (Average Precision): 0.8306\n",
      "\n",
      "--- Final XGB+SMOTE (Top-N=200) ---\n",
      "PR-AUC: 0.8306\n",
      "Precision@Top200: 0.2350 | Recall@Top200: 0.8545 | F1@Top200: 0.3686\n",
      "\n",
      "--- Final XGB+SMOTE (threshold=0.05) ---\n",
      "PR-AUC: 0.8306\n",
      "Precision: 0.3172 | Recall: 0.8364 | F1: 0.4600\n",
      "Confusion Matrix:\n",
      " [[42405    99]\n",
      " [    9    46]]\n",
      "\n",
      "--- FINAL PIPELINE (XGB+SMOTE): TEST ---\n",
      "PR-AUC (Average Precision): 0.7659\n",
      "\n",
      "--- Final XGB+SMOTE (TEST) (Top-N=200) ---\n",
      "PR-AUC: 0.7659\n",
      "Precision@Top200: 0.2050 | Recall@Top200: 0.7885 | F1@Top200: 0.3254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7658630072182174,\n",
       " np.float64(0.205),\n",
       " np.float64(0.7884615384615384),\n",
       " np.float64(0.3253968253968254))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "final_pipeline = ImbPipeline(steps=[\n",
    "    (\"scaler\", RobustScaler()),          # optional for XGB\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"xgb\", XGBClassifier(\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=42,\n",
    "        n_estimators=800,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Validation PR-AUC\n",
    "val_probs = final_pipeline.predict_proba(X_val)[:, 1]\n",
    "val_ap = average_precision_score(y_val, val_probs)\n",
    "print(\"--- FINAL PIPELINE (XGB+SMOTE): VALIDATION ---\")\n",
    "print(f\"PR-AUC (Average Precision): {val_ap:.4f}\")\n",
    "\n",
    "# Operational eval (choose one or both)\n",
    "evaluate_at_top_n(final_pipeline, X_val, y_val, name=\"Final XGB+SMOTE\", top_n=200)\n",
    "evaluate_at_threshold(final_pipeline, X_val, y_val, name=\"Final XGB+SMOTE\", threshold=0.05)\n",
    "\n",
    "# Test PR-AUC\n",
    "test_probs = final_pipeline.predict_proba(X_test)[:, 1]\n",
    "test_ap = average_precision_score(y_test, test_probs)\n",
    "print(\"\\n--- FINAL PIPELINE (XGB+SMOTE): TEST ---\")\n",
    "print(f\"PR-AUC (Average Precision): {test_ap:.4f}\")\n",
    "\n",
    "# Operational test eval\n",
    "evaluate_at_top_n(final_pipeline, X_test, y_test, name=\"Final XGB+SMOTE (TEST)\", top_n=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e9526b",
   "metadata": {},
   "source": [
    "### Model Comparision Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25cf7544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>PR-AUC</th>\n",
       "      <th>Precision@Top200</th>\n",
       "      <th>Recall@Top200</th>\n",
       "      <th>F1@Top200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGB (Weighted)</td>\n",
       "      <td>0.851365</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.376471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RF (SMOTE)</td>\n",
       "      <td>0.850598</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.384314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RF (Under Pipeline)</td>\n",
       "      <td>0.841342</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.376471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGB (SMOTE)</td>\n",
       "      <td>0.832810</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.368627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogReg (SMOTE)</td>\n",
       "      <td>0.820811</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.376471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogReg (Under+Scaled)</td>\n",
       "      <td>0.361482</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>0.360784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GNB (Under)</td>\n",
       "      <td>0.053858</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.109804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model    PR-AUC  Precision@Top200  Recall@Top200  F1@Top200\n",
       "3         XGB (Weighted)  0.851365             0.240       0.872727   0.376471\n",
       "5             RF (SMOTE)  0.850598             0.245       0.890909   0.384314\n",
       "2    RF (Under Pipeline)  0.841342             0.240       0.872727   0.376471\n",
       "6            XGB (SMOTE)  0.832810             0.235       0.854545   0.368627\n",
       "4         LogReg (SMOTE)  0.820811             0.240       0.872727   0.376471\n",
       "0  LogReg (Under+Scaled)  0.361482             0.230       0.836364   0.360784\n",
       "1            GNB (Under)  0.053858             0.070       0.254545   0.109804"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def metrics_at_top_n(model, X, y, top_n):\n",
    "    probs = model.predict_proba(X)[:, 1]\n",
    "    ap = average_precision_score(y, probs)\n",
    "\n",
    "    idx = np.argsort(probs)[::-1]\n",
    "    preds = np.zeros(len(y), dtype=int)\n",
    "    preds[idx[:top_n]] = 1\n",
    "\n",
    "    y_true = np.array(y)\n",
    "\n",
    "    tp = ((preds == 1) & (y_true == 1)).sum()\n",
    "    fp = ((preds == 1) & (y_true == 0)).sum()\n",
    "    fn = ((preds == 0) & (y_true == 1)).sum()\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    return ap, precision, recall, f1\n",
    "\n",
    "TOP_N = 200\n",
    "\n",
    "models = {\n",
    "    \"LogReg (Under+Scaled)\": lr_model,\n",
    "    \"GNB (Under)\": gnb,\n",
    "    \"RF (Under Pipeline)\": rf_under_pipe,\n",
    "    \"XGB (Weighted)\": xgb_weighted,\n",
    "\n",
    "    \"LogReg (SMOTE)\": lr_smote_pipe,\n",
    "    \"RF (SMOTE)\": rf_smote_pipe,\n",
    "    \"XGB (SMOTE)\": xgb_smote_pipe,\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for name, m in models.items():\n",
    "    ap, p, r, f1 = metrics_at_top_n(m, X_val, y_val, TOP_N)\n",
    "    rows.append({\n",
    "        \"Model\": name,\n",
    "        \"PR-AUC\": ap,\n",
    "        f\"Precision@Top{TOP_N}\": p,\n",
    "        f\"Recall@Top{TOP_N}\": r,\n",
    "        f\"F1@Top{TOP_N}\": f1\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(rows).sort_values(\"PR-AUC\", ascending=False)\n",
    "comparison_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8dceea8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model: XGB (Weighted)\n",
      "TEST PR-AUC: 0.7729\n",
      "TEST Precision@Top200: 0.2050\n",
      "TEST Recall@Top200: 0.7885\n",
      "TEST F1@Top200: 0.3254\n"
     ]
    }
   ],
   "source": [
    "best_name = comparison_df.iloc[0][\"Model\"]\n",
    "best_model = models[best_name]\n",
    "\n",
    "ap_t, p_t, r_t, f1_t = metrics_at_top_n(best_model, X_test, y_test, TOP_N)\n",
    "\n",
    "print(\"\\nBest model:\", best_name)\n",
    "print(f\"TEST PR-AUC: {ap_t:.4f}\")\n",
    "print(f\"TEST Precision@Top{TOP_N}: {p_t:.4f}\")\n",
    "print(f\"TEST Recall@Top{TOP_N}: {r_t:.4f}\")\n",
    "print(f\"TEST F1@Top{TOP_N}: {f1_t:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
